{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"new",
				"newbytes"
			],
			[
				"cipe",
				"ciphers"
			],
			[
				"key",
				"keySet"
			],
			[
				"ke",
				"keySet"
			],
			[
				"this",
				"thisCipherText"
			],
			[
				"k",
				"keySet"
			],
			[
				"min",
				"minLen"
			],
			[
				"xo",
				"xored_cipertexts"
			],
			[
				"ci",
				"ciphertexts_range"
			],
			[
				"c",
				"ciphertexts"
			]
		]
	},
	"buffers":
	[
		{
			"file": "_posts/2016-04-09-Simple-efficient-cluster-execution.md",
			"settings":
			{
				"buffer_size": 7359,
				"line_ending": "Unix"
			}
		},
		{
			"file": "spark.tex",
			"settings":
			{
				"buffer_size": 16389,
				"line_ending": "Unix"
			}
		},
		{
			"file": "_config.yml",
			"settings":
			{
				"buffer_size": 2673,
				"line_ending": "Unix"
			}
		},
		{
			"file": "about.md",
			"settings":
			{
				"buffer_size": 478,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "---\nlayout: post\nslug: creating-composable-spark-data-pipelines\ntitle: Composing Spark Data Pipelines\n---\n\nWe investigate a pattern of functional programming that we can apply to [Apache Spark](http://spark.apache.org/) to help create clean and elegant code for data transformations, and avoiding state management headaches. We do so by separating the definition of data transformation processes from their execution. \n\n[Apache Spark](http://spark.apache.org/) is a marvellous piece of software engineering that has taken the big data world by storm. It is currently the most active Scala project and the most active big data project, with a large community of contributors. There are plenty of resources available for finding out more about and learning Spark, and the docs are excellent. This post focuses on some areas of deficiency. These are not necessarily deficiencies in the Spark framework itself, but rather pitfalls that could result in your Spark code getting a little messy without careful planning. This post suggests a pattern that could avoid many of these.\n\nIn going about this, we also describe a functional programming pattern which is also generally applicable in many similar contexts, known as the [Reader monad](http://2016.phillyemergingtech.com/2012/system/presentations/di-without-the-gymnastics.pdf). In spite of this pattern being widely useful, it's not actually that easy to find non-trivial examples which are still easy enough to understand. Hopefully this post will help somewhat. \n\nTo demonstrate, we start off with the canonical example of a MapReduce task, a word count across a large corpus of documents. A simple Spark program may look like this:\n\n```scala\nimport org.apache.spark.{ SparkConf, SparkContext }\n\nobject WordCountOneLiner {\n  def main(args: Array[String]): Unit = {\n\n    val sc = new SparkContext(\n      new SparkConf().setMaster(\"local[2]\").setAppName(\"WordCount\"))\n\n    sc.textFile(Util.baseDir() + \"/TXT/*\")      // Read text files\n      .flatMap { line => line.split(\"\\\\W+\") }   // Split lines into words\n      .map(_.toLowerCase)\n      .filter(!_.isEmpty) \n      .map((_, 1))                              // Create (word,1) pairs\n      .reduceByKey(_ + _)                             \n      .takeOrdered(100)(Ordering.by(-_._2))     // sort descending by count\n\n    sc.stop()                                   // terminate SparkContext\n  }\n}\n```\n\nThis really simple functional code illustrates much of the appeal of Spark, especially for Scala developers, and functional programmers in general.\nAll the operations that define the map and reduce steps are just ordinary scala functions. \n\nIn order to do anything in Spark we need to create a `SparkContext` object. This object is a handle to access and manage the resources required for distributed data processing tasks. It needs to be stopped when finished with to free up resources on the cluster.\n\n## Refactoring into a trait\n\nUnfortunately, real world data programming tasks are never as simple as this simple word count. In addition, we may wish to test each step in the process separately. To cope with this, we refactor our one line code into a trait:\n\n```scala\ntrait WordCountSimple {\n\n  val sparkContext = new SparkContext(\n    new SparkConf().setMaster(\"local[2]\").setAppName(\"WordCount\"))\n\n  def lines = sparkContext.textFile(Util.baseDir() + \"/TXT/*\")\n\n  def words = lines.flatMap { line => line.split(\"\\\\W+\") }\n    .map(_.toLowerCase)\n    .filter(!_.isEmpty)\n\n  def count = words.map((_, 1)).reduceByKey(_ + _)\n\n  def topWords(n: Int) = count.takeOrdered(n)(Ordering.by(-_._2))\n\n  def stop(): Unit = sparkContext.stop()\n}\n```\n\nImmediately design problems become apparent:\n\n* The `SparkContext` object that is part of the trait. What happens if we have several traits, each representing different data processing logic, and we wish to share the `SparkContext` across these traits?\n* Even worse - the `stop()` method that needs to be called when we're done. Who is responsible for doing this, and when?\n\nSo without doing anything non-trivial, we already have a lifecycle and state management headache. \n\nWhat would help is if we could decouple the creation and management of the `SparkContext` from the transformation logic.\n\n## Introducing the SparkOperation monad\n\n```scala\nsealed trait SparkOperation[+A] {\n  // executes the transformations\n  def run(ctx: SparkContext): A\n\n  // enables chaining pipelines, for comprehensions, etc.\n  def map[B](f: A => B): SparkOperation[B] \n  def flatMap[B](f: A => SparkOperation[B]): SparkOperation[B] \n}\n```\n\nand companion object:\n\n```scala\nimport scalaz._\n\nobject SparkOperation {\n  def apply[A](f: SparkContext => A): SparkOperation[A] = new SparkOperation[A] {\n    override def run(ctx: SparkContext): A = f(ctx)\n  }\n\n  implicit val monad = new Monad[SparkOperation] {\n    override def bind[A, B](fa: SparkOperation[A])(f: A ⇒ SparkOperation[B]): SparkOperation[B] =\n      SparkOperation(ctx ⇒ f(fa.run(ctx)).run(ctx))\n\n    override def point[A](a: ⇒ A): SparkOperation[A] = SparkOperation(_ ⇒ a)\n  }\n}\n```\n\nFirstly, to bring everyone onto the same page, what is a *monad*? A *monad* is a key abstration in functional programming. Whenever one is looking for a general solution to composability, monads are normally not too far away. By composability, we mean the output of one process is the input into another. And that is precisely what we are trying to do. A data processing pipeline consists of several operations, each joined together to form the pipeline. And we can join two or more pipelines to form larger pipelines. A monad has this property. Technically, in order to make this possible, a monad must have a `map` and a `flatMap` method. The `flatMap` method is the distinguishing feature of the monad abstractions as it is the operation the enables composability. If you aren't familiar with monads and their associated methods, it takes a little time to get comfortable with them.\n\nIn addition to these methods being present, some algebraic laws need to be satisfied. We don't go into these too deeply, but just mention that these laws simply ensure that things happens as we expect they should. An example is *associativity*. In this context this means that if we have pipelines *A*, *B* and *C*, and we wish to join them to form a single pipeline *ABC*, we can either join *A* and *B* and then join the result *AB* and *C*, or we could join *A* to pipeline *BC*. No need to worry too much about this, suffice to say, the `SparkOperation` satisfies these monad laws, and it's not too hard to verify this. Note that the `SparkOperation` trait is sealed. That means we don't allow other traits or classes to derive from it. This is because we would not be able to ensure that the derived classes still satisfy the same monad laws, and as a result may get used incorrectly.\n\nIf you are familiar with it, you may recognise `SparkOperation` as an example of a [Reader monad](http://2016.phillyemergingtech.com/2012/system/presentations/di-without-the-gymnastics.pdf). We rely on a bit of help from [scalaz](https://github.com/scalaz/scalaz) to form the monad. Rather than using Scalaz, we could have implemented `map` and `flatMap` as follows:\n\n```scala\nsealed trait SparkOperation[+A] {\n  ...\n  def map[B](f: A ⇒ B): SparkOperation[B] = \n  SparkOperation { ctx ⇒ f(this.run(ctx)) }\n  def flatMap[B](f: A ⇒ SparkOperation[B]): SparkOperation[B] = \n  SparkOperation { ctx ⇒ f(this.run(ctx)).run(ctx) }\n}\n```\nHowever, there are benefits to letting Scalaz provide the monad instance, as we will see in some of the functional compostion examples below. \n\n## Rewriting the pipeline using SparkOperations\n\nRewriting our trait in terms of `SparkOperation`s is a simple task.\n\n```scala\ntrait WordCountPipeline {\n\n  // initial SparkOperation created using companion object\n  def linesOp = SparkOperation { sparkContext =>\n    sparkContext.textFile(Util.baseDir() + \"/TXT/*\")\n  }\n\n  // after that we often just need map / flatMap\n  def wordsOp = for (lines <- linesOp) yield {\n    lines.flatMap { line => line.split(\"\\\\W+\") }\n      .map(_.toLowerCase)\n      .filter(!_.isEmpty)\n  }\n\n  def countOp = for (words <- wordsOp) \n    yield words.map((_, 1)).reduceByKey(_ + _)\n\n  def topWordsOp(n: Int): SparkOperation[Map[String, Int]] = \n    countOp.map(_.takeOrdered(n)(Ordering.by(-_._2)).toMap)\n}\n```\n\nIn general, the first operation in the data pipeline normally involves the companion object. That is because we can't start the process without a `SparkContext`. From then on, we often create don't need to explicitly refer to the `SparkContext`, and subsequent operations are created through `map`s, `flatMaps` and other functional operations.\n\nTo execute the pipeline operations, we could write the following code to get a word count of the top 100 words:\n\n```scala\n  val sc = new SparkContext(\n    new SparkConf().setMaster(\"local[2]\").setAppName(\"WordCount\"))\n  val topWordsMap: Map[String, Int] = topWordsOp(100).run(sparkContext)\n  sc.stop()\n```\n\nThe key design accomplishment is we have completely decoupled the definition of the processing logic from the execution of the pipeline. There is no reference to a `SparkContext` instance in the pipeline trait, so we can freely mix and match pipeline traits, take operations from one trait and use their output to form new operations in another trait without worrying about how they would share a `SparkContext`. \n\nHaving this separation gives us a great deal of flexibilty regarding the actual execution. For example, we could execute the same process locally using a short lived `SparkContext`, or on a cluster on a long lived `SparkContext` (how we go about this will be explained in a future post).\n\n## Some examples of functional composition\n\nHere are some examples of how we can apply functional to create new operations from existing one. \n\n### Joins (applicative join operation)\n\nA common operation is a join. Any Spark RDD on a pair `RDD[(K,A)]` has a join method:\n\n```scala\ndef join[W](other: RDD[(K, B)], partitioner: Partitioner): RDD[(K, (A, B))]\n```\n\nWe can lift this join operation into a `SparkOperation` using the `|@|` operator provided by Scalaz:\n\n\n```scala\nimport scalaz.syntax.bind._\n\nobject JoinExample {\n  trait K\n  trait A\n  trait B\n\n  val opA: SparkOperation[RDD[(K,A)]] = ???\n  val opB: SparkOperation[RDD[(K,B)]] = ???\n\n  val joinedOp: SparkOperation[RDD[(K,(A,B))]] = \n    (opA |@| opA)((rddA,rddB) => rddA.join(rddB))\n}\n```\n\n### Sequence operation composed from list\n\nIn this example, we turn an operation that performs a task for a given date to an operation that performs this task for a range of dates using the `sequence` operator.\n\n```scala\nimport scalaz.std.list.listInstance\n\nobject SequenceExample {\n  trait A\n\n  val dateList: List[Date] = ???\n  def opForDate(s: Date) : SparkOperation[A] = ???\n  \n  val opOfLists: SparkOperation[List[A]] = \n    SparkOperation.monad.sequence(dateList map opForDate)  \n}\n```\n\nUsing these patterns we can really clean, functional code that is simple to understand, maintain and extend. We also have a great deal of flexibility in how these jobs are executed, as nothing in the process definition says anything about the the process execution.\n\nThis reader monad pattern naturally extends to any framework or library where operations require an expensive context to be created beforehand. An good example is a database connection. A database connection monad of a similar nature can be used to simplify code for operations that require this connection. \n\n## The Sparkplug library\n\nThe functionality above is available in the [Sparkplug library](https://github.com/springnz/sparkplug). The library includes:\n\n* The `SparkOperation` monad\n* Testing tools for sampling and persisting test data\n* Simple and efficient execution on a cluster\n\n## Testing tools\n\nAmong the testing tools is a very handy set of extensions on `SparkOperation`s. Of particular interest is the extension method:\n\n```scala\ndef sourceFrom(rddName: String, sampler: RDD[A] ⇒ RDD[A] = identitySampler)\n```\n\nThis enables one to save down a data sample to disk. This is really helpful for creating a test data set from data sourced in a database instance. It's typically done by extending the pipeline trait, and overriding the data operation as follows:\n\n```scala\nimport springnz.sparkplug.testkit._\n\ntrait WordCountTestPipeline extends WordCountPipeline {\n  def linesOp = super.linesOp.sourceFrom(\"test_data_set\", sampler)\n}\n```\n\nThe way this works if no test data is present, the original dataset will be used, passed through the sampling function, and the resultant condensed dataset is save into the test resource folder. Once the data is there, there will no longer be any call to the original datasource. \n`sampler` is a RDD sampling function, typically used to reduce the size of the dataset to something manageable for unit tests. A few of these are provided out the box, including a random sampler and sample that takes the first `n` records. \n\nSo it is easy to create test data for unit tests without having any reliance on external data connections in your continuous integration environment. The idea is that if you do a `git pull` followed by `sbt test`, everything should just work.\n\nI hope this has been useful, both in terms of giving you ideas about how you could get the most out of Apache Spark if you use it, and also how this functional pattern could be applied in other development scenarios that you might have.\n\nCluster execution strategies are covered in this post on [executing Spark jobs with Akka]({% post_url 2016-04-09-Simple-efficient-cluster-execution %}).\n\n\n\n\n\n\n\n",
			"file": "_posts/2016-04-05-Creating-Composable-Data-Pipelines-Spark.md",
			"file_size": 13663,
			"file_write_time": 1460185045000000,
			"settings":
			{
				"buffer_size": 13643,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "digraph finite_state_machine {\n  ranksep=0.2;\n  size=\"7,4\"\n  ratio = fill;\n  overlap = false;\n  node [style=filled];\n  node [shape = rectangle];\n  subgraph cluster_0 {\n    label = \"Client Service\";\n    rankdir=UD;\n    node [color = lightblue] \"Application\" ;\n    node [color = lightgreen] \"Sparkplug Client\" \n  }\n  node [color = lightgreen] \"Sparkplug Remote\\n(Driver)\";\n  node [color = lightblue] \"Executor 1\" \"Executor 2\";\n  edge[splines=line,  style=dotted];\n  {rank=same; \"Application\" -> \"Sparkplug Client\" [ label = \"Start\", color=red, style=solid, rank=same];}\n  {rank=same; \"Sparkplug Client\" -> \"Application\" [ label = \"Finish\", color=green, style=solid];}\n  \"Sparkplug Client\" -> \"Sparkplug Remote\\n(Driver)\" [ label = \"Request\", color=red];\n  edge[splines=curved];\n  \"Sparkplug Remote\\n(Driver)\" -> \"Sparkplug Client\" [ label = \"Response\", color=green];\n  edge[splines=curved];\n  \"Sparkplug Remote\\n(Driver)\" -> \"Executor 1\" [ label = \"Task\", color=red];\n  \"Executor 1\" -> \"Sparkplug Remote\\n(Driver)\" [ label = \"Result\", color=green];\n  \"Sparkplug Remote\\n(Driver)\" -> \"Executor 2\" [ label = \"Task\", color=red];\n  \"Executor 2\" -> \"Sparkplug Remote\\n(Driver)\" [ label = \"Result\", color=green];\n}\n\ndigraph g{\nranksep=0.2;\n\nnode[shape=box3d, width=2.3, height=0.6, fontname=\"Arial\"];\nn1[label=\"Incident Commander\"];\nn2[label=\"Public Information\\nOfficer\"];\nn3[label=\"Liaison Officer\"];\nn4[label=\"Safety Officer\"];\nn5[label=\"Operations Section\"];\nn6[label=\"Planning Section\"];\nn7[label=\"Logistics Section\"];\nn8[label=\"Finance/Admin. Section\"];\n\nnode[shape=none, width=0, height=0, label=\"\"];\nedge[dir=none];\nn1 -> p1 -> p2 -> p3;\n{rank=same; n2 -> p1 -> n3;}\n{rank=same; n4 -> p2;}\n{rank=same; p4 -> p5 -> p3 -> p6 -> p7;}\np4 -> n5;\np5 -> n6;\np6 -> n7;\np7 -> n8;\n}\n",
			"settings":
			{
				"buffer_size": 1773,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "{\"query\":\n\n{\n  \"bool\" : {\n    \"must\" : {\n      \"bool\" : {\n        \"must\" : {\n          \"match\" : {\n            \"category_order\" : 0\n          }\n        },\n        \"minimum_should_match\" : \"1\",\n        \"should\" : [ {\n          \"dis_max\" : {\n            \"queries\" : [ {\n              \"constant_score\" : {\n                \"query\" : {\n                  \"match\" : {\n                    \"name.lowercase\" : {\n                      \"query\" : \"newmarket plumbers\",\n                      \"minimum_should_match\" : \"1\"\n                    }\n                  }\n                },\n                \"boost\" : 30000\n              }\n            }, {\n              \"constant_score\" : {\n                \"query\" : {\n                  \"match\" : {\n                    \"name\" : {\n                      \"query\" : \"newmarket plumbers\",\n                      \"minimum_should_match\" : \"1\"\n                    }\n                  }\n                },\n                \"boost\" : 30\n              }\n            }, {\n              \"constant_score\" : {\n                \"query\" : {\n                  \"match\" : {\n                    \"name.english\" : {\n                      \"query\" : \"newmarket plumbers\",\n                      \"minimum_should_match\" : \"1\"\n                    }\n                  }\n                },\n                \"boost\" : 15\n              }\n            }, {\n              \"constant_score\" : {\n                \"query\" : {\n                  \"match\" : {\n                    \"name.start_shingle\" : {\n                      \"query\" : \"newmarket plumbers\",\n                      \"minimum_should_match\" : \"1\"\n                    }\n                  }\n                },\n                \"boost\" : 3000\n              }\n            }, {\n              \"constant_score\" : {\n                \"query\" : {\n                  \"match\" : {\n                    \"name.start_word\" : {\n                      \"query\" : \"newmarket plumbers\",\n                      \"minimum_should_match\" : \"1\"\n                    }\n                  }\n                },\n                \"boost\" : 300\n              }\n            } ]\n          }\n        }, {\n          \"dis_max\" : {\n            \"queries\" : [ {\n              \"constant_score\" : {\n                \"query\" : {\n                  \"match\" : {\n                    \"category_name\" : {\n                      \"query\" : \"newmarket plumbers\",\n                      \"minimum_should_match\" : \"1\"\n                    }\n                  }\n                },\n                \"boost\" : 20\n              }\n            }, {\n              \"constant_score\" : {\n                \"query\" : {\n                  \"match\" : {\n                    \"category_name.start_shingle\" : {\n                      \"query\" : \"newmarket plumbers\",\n                      \"minimum_should_match\" : \"1\"\n                    }\n                  }\n                },\n                \"boost\" : 2000\n              }\n            }, {\n              \"constant_score\" : {\n                \"query\" : {\n                  \"match\" : {\n                    \"category_name.start_word\" : {\n                      \"query\" : \"newmarket plumbers\",\n                      \"minimum_should_match\" : \"1\"\n                    }\n                  }\n                },\n                \"boost\" : 200\n              }\n            }, {\n              \"constant_score\" : {\n                \"query\" : {\n                  \"match\" : {\n                    \"category_name.english\" : {\n                      \"query\" : \"newmarket plumbers\",\n                      \"minimum_should_match\" : \"1\"\n                    }\n                  }\n                },\n                \"boost\" : 10\n              }\n            }, {\n              \"constant_score\" : {\n                \"query\" : {\n                  \"match\" : {\n                    \"category_name.lowercase\" : {\n                      \"query\" : \"newmarket plumbers\",\n                      \"minimum_should_match\" : \"1\"\n                    }\n                  }\n                },\n                \"boost\" : 20000\n              }\n            } ]\n          }\n        } ]\n      }\n    },\n    \"filter\" : {\n      \"geo_bounding_box\" : {\n        \"contact.location\" : {\n          \"top_left\" : {\n            \"lat\" : -26.79239316830072,\n            \"lon\" : 164.6946669492188\n          },\n          \"bottom_right\" : {\n            \"lat\" : -46.90448514615546,\n            \"lon\" : 179.8319960507813\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\n}",
			"settings":
			{
				"buffer_size": 4392,
				"line_ending": "Unix",
				"name": "{\"query\":"
			}
		},
		{
			"file": "/Users/stephen/Documents/Documents/Personal/CV/stephen.tex",
			"settings":
			{
				"buffer_size": 11839,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tikz.tex",
			"settings":
			{
				"buffer_size": 1980,
				"line_ending": "Unix",
				"name": "% (S) -> (M) -> (V) -> (shuffle) -> (P) -> (R) - ("
			}
		},
		{
			"file": "_posts/2016-04-02-Covariance-and-Blood-Groups.md",
			"settings":
			{
				"buffer_size": 5152,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"command_palette":
	{
		"height": 347.0,
		"selected_items":
		[
			[
				"Package Control: ",
				"Package Control: Install Package"
			]
		],
		"width": 449.0
	},
	"console":
	{
		"height": 125.0
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"file_history":
	[
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_posts/2016-04-04-Simple-efficient-cluster-execution.md",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_posts.new/2016-04-04-Simple-efficient-cluster-execution.md",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_layouts/post.html",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_config.yml",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/index.html",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_site/style.css",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_posts/2016-04-03-Creating-Composable-Data-Pipelines-Spark.md",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/archive.md",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_posts/2016-04-02-Covariance-and-Blood-Groups.md",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_layouts/default.html",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_layouts/page.html",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_posts/2016-04-05-Creating-Composable-Data-Pipelines-Spark.md",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/about.md",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/CNAME",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_posts.new/2016-04-02-Watch-This-Space.md",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_posts/2016-04-02-Watch-This-Space.md",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_posts/2014-3-3-Hello-World.md",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/css/main.scss",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/_posts/2015-08-15-functional-dependency-injection-with-macwire.markdown",
		"/Users/stephen/.sbt/0.13/plugins/scalariform.sbt",
		"/Users/stephen/Documents/Code/Spring/neon-data/scalariform.sbt",
		"/Users/stephen/Documents/Code/Blog/szoio.github.io/project.sublime-project",
		"/Users/stephen/Documents/Code/Scala/fpinscala/exercises/src/main/scala/fpinscala/gettingstarted/GettingStarted.scala",
		"/Users/stephen/show_hidden_files_yes.sh",
		"/Users/stephen/.zshrc",
		"/Users/stephen/Learning/Coursera/Cryptography/Assignments/generate_ciphers.py",
		"/Users/stephen/Learning/Coursera/Cryptography/Assignments/decrypt_otp.py"
	],
	"find":
	{
		"height": 35.0
	},
	"find_in_files":
	{
		"height": 0.0,
		"where_history":
		[
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			"vandelay",
			"Jar",
			")\n",
			"keySet"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
			"newmarket plumbers"
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 7,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "_posts/2016-04-09-Simple-efficient-cluster-execution.md",
					"settings":
					{
						"buffer_size": 7359,
						"regions":
						{
						},
						"selection":
						[
							[
								17,
								23
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "spark.tex",
					"settings":
					{
						"buffer_size": 16389,
						"regions":
						{
						},
						"selection":
						[
							[
								394,
								174
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "_config.yml",
					"settings":
					{
						"buffer_size": 2673,
						"regions":
						{
						},
						"selection":
						[
							[
								229,
								229
							]
						],
						"settings":
						{
							"syntax": "Packages/YAML/YAML.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "about.md",
					"settings":
					{
						"buffer_size": 478,
						"regions":
						{
						},
						"selection":
						[
							[
								478,
								478
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "_posts/2016-04-05-Creating-Composable-Data-Pipelines-Spark.md",
					"settings":
					{
						"buffer_size": 13643,
						"regions":
						{
						},
						"selection":
						[
							[
								2511,
								2511
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 5,
					"settings":
					{
						"buffer_size": 1773,
						"regions":
						{
						},
						"selection":
						[
							[
								1206,
								1206
							]
						],
						"settings":
						{
							"syntax": "Packages/Graphviz/DOT.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 6,
					"settings":
					{
						"buffer_size": 4392,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								4392
							]
						],
						"settings":
						{
							"auto_name": "{\"query\":",
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1651.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "/Users/stephen/Documents/Documents/Personal/CV/stephen.tex",
					"settings":
					{
						"buffer_size": 11839,
						"regions":
						{
						},
						"selection":
						[
							[
								2410,
								2410
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 253.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 8,
					"file": "tikz.tex",
					"settings":
					{
						"buffer_size": 1980,
						"regions":
						{
						},
						"selection":
						[
							[
								198,
								198
							]
						],
						"settings":
						{
							"auto_name": "% (S) -> (M) -> (V) -> (shuffle) -> (P) -> (R) - (",
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "_posts/2016-04-02-Covariance-and-Blood-Groups.md",
					"settings":
					{
						"buffer_size": 5152,
						"regions":
						{
						},
						"selection":
						[
							[
								54,
								54
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 0.0
	},
	"input":
	{
		"height": 31.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"replace":
	{
		"height": 64.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 0.0,
		"selected_items":
		[
		],
		"width": 0.0
	},
	"show_minimap": true,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 290.0,
	"status_bar_visible": true
}
